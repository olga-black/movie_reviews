{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:turquoise\">Text classification with pytorch</span>\n",
    "\n",
    "\n",
    "An example of using natural language processing for sentiment analysis. <br> Building a binary classifier of movie reviews that will predict if a review is positive or negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Dataset:__ IMDB movie reviews from Kaggle<br>\n",
    "__Model:__ LSTM (?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:teal\">Todo:</span>\n",
    "\n",
    "- ~~Read dataset~~\n",
    "- ~~Preprocess text~~\n",
    "- ~~Split into train, validation, and test sets~~\n",
    "- ~~Convert text to indices and add paddings~~\n",
    "- ~~Make model~~\n",
    "- Make training function\n",
    "- Make evaluation function\n",
    "- Train\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Read the data and split it into training, cross-validation, and test sets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = {}\n",
    "        self.val = {}\n",
    "        self.test = {}\n",
    "        self.LABELS = {\"positive\":1, \"negative\": 0}\n",
    "        self.COUNT = {\"positive\": 0, \"negative\": 0}\n",
    "    \n",
    "    \n",
    "    def read_data(self):\n",
    "        \n",
    "        dataset = []\n",
    "        \n",
    "        with open (\"IMDB_Dataset.csv\", newline='') as f:\n",
    "            datareader = csv.reader(f, delimiter=',')\n",
    "            next(datareader, None)\n",
    "\n",
    "            for row in datareader:\n",
    "                dataset.append([row[0], self.LABELS[row[1]]])\n",
    "                self.COUNT[row[1]] += 1\n",
    "            \n",
    "            random.shuffle(dataset)\n",
    "                \n",
    "        return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(self,\n",
    "                      dataset,\n",
    "                      split=[int(50000*0.6), int(50000*0.2), int(50000*0.2)]):\n",
    "        \n",
    "        train, val, test = torch.utils.data.random_split(dataset,\n",
    "                                               split,\n",
    "                                               generator=torch.Generator().manual_seed(43))\n",
    "          \n",
    "            \n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = Reviews()\n",
    "data = rev.read_data()\n",
    "pos_count = rev.COUNT[\"positive\"]\n",
    "neg_count = rev.COUNT[\"negative\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Well since seeing part's 1 through 3 I can honestly say that they should have NEVER made part 4. Everything from the tacky, and I DO mean tacky score to the really bad acting, I dare anyone to watch this and not be bored out of their minds. I mean parts 1 to 3 kept the vibe strong on the plot of Damion, but without him around in this one it's just not the same. Sure by the end of part 3 I was getting a little tired of the continued story line's, but it was a good closure at the end of the third one. Again there was no reason for a part 4. Even if there was they could have done a MUCH better job than this sh*t I had to sit through, lol. There goes an hour and a half of my life i'll never see again.\", 0]\n"
     ]
    }
   ],
   "source": [
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Okay, I've tried and I've tried, but I STILL DON'T GET this Guy Maddin thing. Tales From the Gimli Hospital left me cold, that movie about the Austrian villagers and the one about the Ice Nymph were pretty to look but lacking in the story department...and this nudie movie about abortion and hockey is just boring. I'm glad Maddin has an appreciation for silent film, but I dislike his films for the same reason I dislike the films of Quentin Tarantino: they're empty homages to better, more imaginative films--films that advanced the art form or broke new ground--and are all style and no substance. No amount of jump cuts and odd camera angles can disguise the fact that Maddin is an unoriginal David Lynch wannabe, though he DOES have one advantage over Tarantino: he generally doesn't write embarrassing dialogue, because most of his films rely on intertitles. The bottom line is, Maddin's schtick is clever clever film-making for aspiring film majors.\", 0]\n"
     ]
    }
   ],
   "source": [
    "train, val, test = rev.split_dataset(data)\n",
    "print(train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_and_y(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for review, label in data:\n",
    "        x.append(review)\n",
    "        y.append(label)\n",
    "    return x, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000\n",
      "I like to think of myself as a bad movie connoisseur. I like to think that the films most people label as the worst of all time I can easily withstand. So...from one bad movie fan to another...let this collect dust on the shelf...grab Up From the Depths or The Great Alligator instead to satisfy your need for something evil lurking in the water. 0\n"
     ]
    }
   ],
   "source": [
    "train_x_raw, train_y = split_x_and_y(train)\n",
    "val_x_raw, val_y = split_x_and_y(val)\n",
    "test_x_raw, test_y = split_x_and_y(test)\n",
    "\n",
    "\n",
    "print(len(train_x_raw), len(train_y))\n",
    "print(train_x_raw[50], train_y[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Preprocess text</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review,\n",
    "               remove_stopwords=False, \n",
    "               remove_html=True, \n",
    "               remove_punct=False, \n",
    "               lowercase=False, \n",
    "               lemmatize=False,\n",
    "               maxlen=128):\n",
    "    \n",
    "    review = re.sub(r\"\\'\", \"'\", review)\n",
    "    review = re.sub(r\"\\x96\", \"-\", review)\n",
    "    \n",
    "    if remove_html:\n",
    "        review = re.sub(r'<.*>', ' ', review)\n",
    "    \n",
    "    review = word_tokenize(review)\n",
    "        \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        review = [w for w in review if w not in stop_words]\n",
    "        \n",
    "    if remove_punct:\n",
    "        contractions = [\"'ll\", \"'s\", \"n't\", \"'d\", \"'m\", \"'ve\", \"'re\"]\n",
    "        review = [w for w in review if w.isalnum() or w in contractions]\n",
    "    \n",
    "    if lowercase:\n",
    "        review = [w.lower() for w in review]\n",
    "        \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(w) for w in review]\n",
    "    \n",
    "    \n",
    "    return review[:maxlen]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [preprocess(review, \n",
    "                      lowercase=True, \n",
    "                      remove_punct=True,\n",
    "                      remove_stopwords=True\n",
    "                     ) \n",
    "           for review in train_x_raw]\n",
    "\n",
    "val_words = [preprocess(review, \n",
    "                    lowercase=True, \n",
    "                    remove_punct=True,\n",
    "                    remove_stopwords=True\n",
    "                   ) \n",
    "         for review in val_x_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'comment', 'mainly', 'comment', 'first', 'commentator', 'extra', 'film', 'unhappy', 'assessment', 'film', 'i', 'think', 'perspective', 'indicates', 'extra', 'extra', 'director', 'director', 'the', 'film', 'sweet', 'acting', 'sufficient', 'experience', 'watching', 'nice', 'diversion', 'busy', 'work', 'week', 'it', \"n't\", 'the', 'hours', 'acting', 'the', 'matrix', 'special', 'effects', 'even', 'the', 'color', 'purple', 'direction', 'most', 'movies', 'wo', \"n't\", 'but', 'also', \"n't\", 'crap', 'fest', 'vinny', 'would', 'lead', 'believe', 'sorry', 'guy', '2', 'cents', 'as', 'movie', 'end', 'gay', 'affirming', '1', 'it', 'showed', 'world', 'full', 'diverse', 'less', 'perfect', 'people', 'know', 'like', '2', 'it', 'opened', 'door', 'one', 'culture', 'without', 'excluding', 'cultures', '3', 'and', 'i', 'liked', 'music', '4'] \n",
      " ['in', 'bygone', 'days', 'catholic', 'church', 'individual', 'ritual', 'would', 'take', 'sins', 'dying', 'person', 'upon', 'often', 'people', 'excommunicate', 'similar', 'individuals', 'church', 'would', 'absolve', 'thereby', 'denying', 'entrance', 'heaven', 'the', 'seen', 'blasphemous', 'circumventing', 'chruch', \"'s\", 'monopoly', 'redemption', 'sex', 'bit', 'overt', 'supernatural', 'mojo', 'let', 'concept', 'wander', 'may', 'the', 'order', 'movie', 'combines', 'stigmata', \"'s\", 'religious', 'the', 'paranormal', 'investigation', 'the', 'thorn', 'birds', 'sexual', 'spirituality', 'odd', 'melange', 'sometimes', 'works', 'a', 'confusing', 'plot', 'lack', 'purpose', 'sometimes', 'sleepy', 'performances', 'would', 'often', 'damn', 'movie', 'reason', 'the', 'order', 'remains', 'watchable', 'many', 'people', 'turned', 'movie', 'odd', 'sensibilities', 'may', 'even', 'become', 'angry', 'forced', 'engage', 'higher', 'functions', 'brain', 'understand', 'still', 'film', \"'s\", 'sheer', 'intangibility', 'prevent', 'either', 'critical', 'commercial', 'success', 'dvd', 'i', \"'m\", 'sure', 'stocked', 'copious', 'amounts', 'deleted', 'scenes', 'a', 'recommended', 'film', 'people', 'like', 'think', 'watch', '6', '10']\n",
      "93 \n",
      " 122\n"
     ]
    }
   ],
   "source": [
    "print(train_words[9592], '\\n', val_words[3029])\n",
    "print(len(train_words[9592]), '\\n', len(val_words[3029]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Convert text to indices and add paddings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary_dicts(preprocessed_data, pad_token='<PAD>', unk_token='<UNK>'):\n",
    "    vocab = set()\n",
    "    \n",
    "    for review in preprocessed_data:\n",
    "        for word in review:\n",
    "            vocab.add(word)\n",
    "    \n",
    "            \n",
    "    vocab_sorted = sorted(vocab)\n",
    "    word2ind = {word : i+2 for i, word in enumerate(vocab_sorted)}\n",
    "    ind2word = {i+2 : word for i, word in enumerate(vocab_sorted)}\n",
    "    \n",
    "    # Prepend the pad token\n",
    "    word2ind[pad_token] = 0\n",
    "    ind2word[0] = pad_token\n",
    "    \n",
    "    # Prepend the 'unknown' token\n",
    "    word2ind[unk_token] = 1\n",
    "    ind2word[1] = unk_token\n",
    "    \n",
    "    assert len(word2ind) == len(ind2word)\n",
    "\n",
    "    \n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_raw, val_x_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61370 61370\n",
      "37288 4133\n",
      "booked clutches\n"
     ]
    }
   ],
   "source": [
    "word2ind, ind2word = make_vocabulary_dicts(train_words)\n",
    "\n",
    "print(len(word2ind), len(ind2word))\n",
    "print(word2ind['never'], word2ind['awful'])\n",
    "print(ind2word[6700], ind2word[10582])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "71.2575\n",
      "128\n",
      "70.8972\n"
     ]
    }
   ],
   "source": [
    "print(np.max([len(x) for x in train_words]))\n",
    "print(np.mean([len(x) for x in train_words]))\n",
    "\n",
    "print(np.max([len(x) for x in val_words]))\n",
    "print(np.mean([len(x) for x in val_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_inputs(preprocessed_data, \n",
    "                       vocab, \n",
    "                       padded_length=128,\n",
    "                       pad_token='<PAD>',\n",
    "                       unk_token='<UNK>'\n",
    "                      ):\n",
    "    \n",
    "    num_lines = len(preprocessed_data)\n",
    "    pad = vocab[pad_token]\n",
    "    \n",
    "    unpadded_lengths = np.zeros(num_lines, dtype='int64')\n",
    "    \n",
    "    inputs = np.full((num_lines, padded_length), pad)\n",
    "    \n",
    "    for i, review in enumerate(preprocessed_data):\n",
    "        for j, word in enumerate(review):    \n",
    "            inputs[i, j] = vocab.get(word, vocab[unk_token])\n",
    "        unpadded_lengths[i] = j+1\n",
    "            \n",
    "    return inputs, unpadded_lengths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example at index 10:\n",
      "['okay', 'i', \"'ve\", 'tried', 'i', \"'ve\", 'tried', 'i', 'still', 'do', 'get', 'guy', 'maddin', 'thing', 'tales', 'from', 'gimli', 'hospital', 'left', 'cold', 'movie', 'austrian', 'villagers', 'one', 'ice', 'nymph', 'pretty', 'look', 'lacking', 'story', 'department', 'nudie', 'movie', 'abortion', 'hockey', 'boring', 'i', \"'m\", 'glad', 'maddin', 'appreciation', 'silent', 'film', 'i', 'dislike', 'films', 'reason', 'i', 'dislike', 'films', 'quentin', 'tarantino', \"'re\", 'empty', 'homages', 'better', 'imaginative', 'films', 'films', 'advanced', 'art', 'form', 'broke', 'new', 'ground', 'style', 'substance', 'no', 'amount', 'jump', 'cuts', 'odd', 'camera', 'angles', 'disguise', 'fact', 'maddin', 'unoriginal', 'david', 'lynch', 'wannabe', 'though', 'does', 'one', 'advantage', 'tarantino', 'generally', \"n't\", 'write', 'embarrassing', 'dialogue', 'films', 'rely', 'intertitles', 'the', 'bottom', 'line', 'maddin', \"'s\", 'schtick', 'clever', 'clever', 'aspiring', 'film', 'majors']\n",
      "\n",
      "    Converted to indices:\n",
      "[38433 26543     7 56041 26543     7 56041 26543 51970 15812 22413 23959\n",
      " 32970 54679 53748 21452 22579 26054 31380 10790 36240  3949 58611 38549\n",
      " 26575 38052 42419 32312 30727 52143 14397 37934 36240   891 25617  6816\n",
      " 26543     4 22685 32970  3048 49432 20105 26543 15460 20137 44232 26543\n",
      " 15460 20137 43504 53904     5 17752 25756  5659 26797 20137 20137  1443\n",
      "  3396 20958  7423 37302 23639 52509 52631 37607  2385 29354 13168 38291\n",
      "  8287  2600 15408 19265 32970 57475 13603 32785 59171 54759 15879 38549\n",
      "  1448 53904 22242 36720 60585 17602 14895 20137 44915 28076 54542  6886\n",
      " 31906 32970     6 47692 10383 10383  3561 20105 33180     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      " \n",
      "    Unpadded length of the example:\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "train_x, train_lengths = make_padded_inputs(train_words, word2ind)\n",
    "val_x, val_lengths = make_padded_inputs(val_words, word2ind)\n",
    "\n",
    "\n",
    "print(f\"\"\"Training example at index 10:\\n{train_words[10]}\\n\n",
    "    Converted to indices:\\n{train_x[10, :]}\\n \n",
    "    Unpadded length of the example:\\n{train_lengths[10]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Load data into torch</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Create model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 d_feature, \n",
    "                 num_layers, \n",
    "                 hidden_size,\n",
    "                 n_outputs,\n",
    "                 bidirectional=False,\n",
    "                 dropout_rate=0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_feature)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.lstm = nn.LSTM(input_size=d_feature,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_outputs)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_data): \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input_data))\n",
    "        #print(f'{embedded.shape=}')\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        #print(f'{lstm_out.shape=}')\n",
    "        \n",
    "        fc = self.fc(lstm_out[:,-1,:])\n",
    "        #print(f'{fc.shape=}')\n",
    "        sigmoid = self.sigmoid(fc)\n",
    "        #print(f'{sigmoid.shape=}')\n",
    "   \n",
    "        return sigmoid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentClassifier(\n",
      "  (embedding): Embedding(61370, 128)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2ind)\n",
    "d_feature = 128\n",
    "hidden_size = 128\n",
    "n_outputs = 1\n",
    "num_layers = 1\n",
    "\n",
    "model = SentimentClassifier(\n",
    "                            vocab_size=vocab_size, \n",
    "                            d_feature=d_feature,  \n",
    "                            num_layers=num_layers, \n",
    "                            hidden_size=hidden_size, \n",
    "                            n_outputs=n_outputs).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader=train_loader,\n",
    "             val_loader=val_loader,\n",
    "             model=model,\n",
    "             train_lengths=train_lengths,\n",
    "             val_lengths=val_lengths,\n",
    "             optimizer=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "             criterion=nn.BCELoss(),\n",
    "             n_epochs=5,\n",
    "             print_every=22):\n",
    "    \n",
    "    train_step = 0\n",
    "    val_step = 0 \n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:  \n",
    "            model.zero_grad()\n",
    "            #lengths = train_lengths[batch_size*step : batch_size*(step+1)]\n",
    "            output = model(inputs)\n",
    "            #print(labels.shape)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            train_step +=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "           \n",
    "        model.eval()\n",
    "            \n",
    "        val_losses = []\n",
    "        \n",
    "        \n",
    "        for val_inputs, val_labels in val_loader:\n",
    "\n",
    "            #v_lengths = val_lengths[batch_size*step : batch_size*(step+1)]\n",
    "            val_output = model(val_inputs)\n",
    "            #print(f'val_labels shape:{val_labels.shape}')\n",
    "            val_loss = criterion(val_output.squeeze(), val_labels.float())\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        if (val_step % print_every) == 0:\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Time taken: {:.2f} min\".format((time.time() - start_time) // 60),\n",
    "                  \"Step: {}\".format(val_step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "            val_step +=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
