{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:turquoise\">Text classification with pytorch</span>\n",
    "\n",
    "\n",
    "An example of using natural language processing for sentiment analysis. <br> Building a binary classifier of movie reviews that will predict if a review is positive or negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Dataset:__ IMDB movie reviews from Kaggle<br>\n",
    "__Model:__ LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:teal\">Todo:</span>\n",
    "\n",
    "- ~~Read dataset~~\n",
    "- ~~Preprocess text~~\n",
    "- ~~Split into train, validation, and test sets~~\n",
    "- ~~Convert text to indices and add paddings~~\n",
    "- ~~Make model~~\n",
    "- ~~Make training function~~\n",
    "- ~~Make evaluation function~~\n",
    "- ~~Train~~\n",
    "- Evaluate on test set\n",
    "- Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Read the data and split it into training, cross-validation, and test sets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = {}\n",
    "        self.val = {}\n",
    "        self.test = {}\n",
    "        self.LABELS = {\"positive\":1, \"negative\": 0}\n",
    "        self.COUNT = {\"positive\": 0, \"negative\": 0}\n",
    "    \n",
    "    \n",
    "    def read_data(self):\n",
    "        \n",
    "        dataset = []\n",
    "        \n",
    "        with open (\"IMDB_Dataset.csv\", newline='') as f:\n",
    "            datareader = csv.reader(f, delimiter=',')\n",
    "            next(datareader, None)\n",
    "\n",
    "            for row in datareader:\n",
    "                dataset.append([row[0], self.LABELS[row[1]]])\n",
    "                self.COUNT[row[1]] += 1\n",
    "            \n",
    "            random.shuffle(dataset)\n",
    "                \n",
    "        return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(self,\n",
    "                      dataset,\n",
    "                      split=[int(50000*0.6), int(50000*0.2), int(50000*0.2)]):\n",
    "        \n",
    "        train, val, test = torch.utils.data.random_split(dataset,\n",
    "                                               split,\n",
    "                                               generator=torch.Generator().manual_seed(43))\n",
    "          \n",
    "            \n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = Reviews()\n",
    "data = rev.read_data()\n",
    "pos_count = rev.COUNT[\"positive\"]\n",
    "neg_count = rev.COUNT[\"negative\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hungary can\\'t make any good movies. Fact. This is a great example of that. Someone please explain it to me, why critics say this movie is a masterpiece. Calling this an \"Art\" isn\\'t gonna make it better. Sorry Mundruczo, but you failed. Live with it. Even tho you probably won\\'t care about my or any other guys opinion scarifying your \"child\".', 0]\n"
     ]
    }
   ],
   "source": [
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Horror-genius Dario Argento is one of my personal favorite directors, and his films \"Suspiria\", \"Phenomena\" and \"Profondo Rosso\" range high on my personal all-time favorite list. \"Opera\" of 1987 is yet another tantalizing and brilliant film that no Horror lover can afford to miss, and that will keep you on the edge of your chair from the beginning to the end. This stunning and ultra-violent Giallo could well be described as the master\\'s nastiest film, which is quite something considering that Argento\\'s films are not exactly known for the tameness of their violence. The violence is extreme and very stylized in a brilliant way that makes Opera a film censor\\'s nightmare. As usual for Argento\\'s films, the violence is extremely graphic and very stylized. \"Opera\" truly is a brutal film, and what a stylish and atmospheric film it is. This film is absolutely tantalizing and pure suspense from the beginning to the end. The performances are entirely very good, especially Christina Marsillach is brilliant in the lead. A stunning beauty and great actress alike, Marsillach fits perfectly in her role of the talented singer, whose fear and horrid experiences are slowly making her crazy. Other great performances include those of Ian Charleston as a Horror film director who is directing an Opera, and director Argento\\'s real-life girlfriend Daria Nicolodi, who has a role in many of his movies. The camera work is excellent as in all Argento films and The huge Opera House is an excellent setting that contributes a lot both to the film\\'s beauty and its permanently creepy atmosphere. The score, which is partly classical music and partly heavy metal is great too, even though I slightly missed Goblin\\'s brilliant Progressive Rock Soundtracks that are such a distinguishing element of most other Argento movies. \"Opera\" truly is a terrifying and absolutely breathtaking Giallo experience. This is an absolute must-see for any Horror lover, and I highly recommend it to any other film-fan who is not too sensitive when it comes to extreme violence. Excellent and absolutely tantalizing!', 1]\n"
     ]
    }
   ],
   "source": [
    "train, val, test = rev.split_dataset(data)\n",
    "print(train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_and_y(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for review, label in data:\n",
    "        x.append(review)\n",
    "        y.append(label)\n",
    "    return x, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000\n",
      "Look no further, this is it, the worst movie ever made. There may be others that are tied, but there are none worse. There can't be. The guidelines ask that you \"focus on the content and context\". I can't. There isn't enough content to focus on, and that's exactly my point. Sometimes bad is just bad, and this movie would have to be much better than it is to aspire to being only that. 0\n"
     ]
    }
   ],
   "source": [
    "train_x_raw, train_y = split_x_and_y(train)\n",
    "val_x_raw, val_y = split_x_and_y(val)\n",
    "test_x_raw, test_y = split_x_and_y(test)\n",
    "\n",
    "\n",
    "print(len(train_x_raw), len(train_y))\n",
    "print(train_x_raw[50], train_y[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Preprocess text</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review,\n",
    "               remove_stopwords=False, \n",
    "               remove_html=True, \n",
    "               remove_punct=False, \n",
    "               lowercase=False, \n",
    "               lemmatize=False,\n",
    "               maxlen=128):\n",
    "    \n",
    "    review = re.sub(r\"\\'\", \"'\", review)\n",
    "    review = re.sub(r\"\\x96\", \"-\", review)\n",
    "    \n",
    "    if remove_html:\n",
    "        review = re.sub(r'<.*>', ' ', review)\n",
    "    \n",
    "    review = word_tokenize(review)\n",
    "        \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        review = [w for w in review if w not in stop_words]\n",
    "        \n",
    "    if remove_punct:\n",
    "        contractions = [\"'ll\", \"'s\", \"n't\", \"'d\", \"'m\", \"'ve\", \"'re\"]\n",
    "        review = [w for w in review if w.isalnum() or w in contractions]\n",
    "    \n",
    "    if lowercase:\n",
    "        review = [w.lower() for w in review]\n",
    "        \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(w) for w in review]\n",
    "    \n",
    "    \n",
    "    return review[:maxlen]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [preprocess(review, \n",
    "                      lowercase=True, \n",
    "                      remove_punct=True,\n",
    "                      remove_stopwords=True\n",
    "                     ) \n",
    "           for review in train_x_raw]\n",
    "\n",
    "val_words = [preprocess(review, \n",
    "                    lowercase=True, \n",
    "                    remove_punct=True,\n",
    "                    remove_stopwords=True\n",
    "                   ) \n",
    "         for review in val_x_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['michael', 'jackson', 'would', 'claimed', 'spot', 'character', 'the', 'golden', 'child', 'loves', 'kids', 'that', \"n't\", 'work', 'instead', 'eddie', 'murphy', 'save', 'world', 'rescuing', 'kid', 'midas', 'i', 'would', 'strongly', 'suggest', 'future', 'scriptwriters', 'please', 'thoroughly', 'study', 'actor', \"'s\", 'inane', 'dialogue', 'quirky', 'fantasy', 'adventure', 'comedy', \"'s\", 'step', 'closer', 'ishtar', 'whatever', 'murphy', 'says', 'best', 'liked', \"n't\", 'get', 'wrong', 'exquisite', 'comical', 'talent', \"n't\", 'belong', 'movie', 'went', 'dolittle', 'the', 'violence', 'visuals', 'combined', 'reasons', 'stamp', 'cult', 'camp', 'classic', 'would', \"n't\", 'made', 'sense', 'hollywood', 'movie', 'fanatics', 'kept', 'cashing', 'guy', 'speaking', 'visuals', 'pulled', 'amazingly', 'well', 'time', 'ronald', 'reagan', \"'s\", 'presidential', 'fame', 'murphy', 'far', 'better', 'coming', 'to', 'america', '48', 'hrs', 'stale', 'movie', \"n't\", 'touch', 'golden', 'honey', 'sweet', 'crunchy', 'taste'] \n",
      " ['if', 'film', 'rated', 'scale', '1', '10', 'one', 'would', 'need', 'create', 'new', 'rating', 'system', 'one', 'even', 'qualify', 'the', 'film', \"'s\", 'plot', 'call', 'revolves', 'around', 'charlie', 'stephen', 'baldwin', 'special', 'operatives', 'agent', 'targeted', 'brother', 'man', 'killed', 'still', 'working', 'us', 'if', 'sounds', 'like', 'interesting', 'scenario', 'please', \"n't\", 'fooled', 'film', 'deliver', 'story', 'suggests', 'if', 'type', 'person', 'enjoys', 'watching', 'bad', 'films', 'laughing', 'shortcomings', 'film', 'however', 'looking', 'well', 'made', 'action', 'thriller', 'would', 'best', 'look', 'somewhere', 'else', 'rather', 'renting', 'film']\n",
      "106 \n",
      " 72\n"
     ]
    }
   ],
   "source": [
    "print(train_words[9592], '\\n', val_words[3029])\n",
    "print(len(train_words[9592]), '\\n', len(val_words[3029]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Convert text to indices and add paddings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary_dicts(preprocessed_data, pad_token='<PAD>', unk_token='<UNK>'):\n",
    "    vocab = set()\n",
    "    \n",
    "    for review in preprocessed_data:\n",
    "        for word in review:\n",
    "            vocab.add(word)\n",
    "    \n",
    "            \n",
    "    vocab_sorted = sorted(vocab)\n",
    "    word2ind = {word : i+2 for i, word in enumerate(vocab_sorted)}\n",
    "    ind2word = {i+2 : word for i, word in enumerate(vocab_sorted)}\n",
    "    \n",
    "    # Prepend the pad token\n",
    "    word2ind[pad_token] = 0\n",
    "    ind2word[0] = pad_token\n",
    "    \n",
    "    # Prepend the 'unknown' token\n",
    "    word2ind[unk_token] = 1\n",
    "    ind2word[1] = unk_token\n",
    "    \n",
    "    assert len(word2ind) == len(ind2word)\n",
    "\n",
    "    \n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_raw, val_x_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61449 61449\n",
      "37522 4173\n",
      "bondarchuk cloakroom\n"
     ]
    }
   ],
   "source": [
    "word2ind, ind2word = make_vocabulary_dicts(train_words)\n",
    "\n",
    "print(len(word2ind), len(ind2word))\n",
    "print(word2ind['never'], word2ind['awful'])\n",
    "print(ind2word[6700], ind2word[10582])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "71.141\n",
      "128\n",
      "71.329\n"
     ]
    }
   ],
   "source": [
    "print(np.max([len(x) for x in train_words]))\n",
    "print(np.mean([len(x) for x in train_words]))\n",
    "\n",
    "print(np.max([len(x) for x in val_words]))\n",
    "print(np.mean([len(x) for x in val_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_inputs(preprocessed_data, \n",
    "                       vocab, \n",
    "                       padded_length=128,\n",
    "                       pad_token='<PAD>',\n",
    "                       unk_token='<UNK>'\n",
    "                      ):\n",
    "    \n",
    "    num_lines = len(preprocessed_data)\n",
    "    pad = vocab[pad_token]\n",
    "    \n",
    "    inputs = np.full((num_lines, padded_length), pad)\n",
    "    \n",
    "    for i, review in enumerate(preprocessed_data):\n",
    "        start_position = padded_length - len(review)\n",
    "        for j, word in enumerate(review):\n",
    "            inputs[i, (start_position + j)] = vocab.get(word, vocab[unk_token])\n",
    "            \n",
    "    return inputs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example at indices 5 and 6:\n",
      "[['i', 'think', 'consensus', 'pretty', 'unanimous', 'recent', 'tv', 'miniseries', \"'s\", 'okay', \"'s\", 'far', 'cry', 'lonesome', 'dove', 'it', 'gets', 'compared', 'latter', 'simply', 'prequel', 'famous', 'story', 'note', 'the', 'title', 'page', 'says', '360', 'minutes', 'that', 'must', 'included', 'tv', 'commercials', 'the', 'dvd', 'version', 'i', 'saw', '4', 'hours', '40', 'minutes'], ['the', 'late', 'great', 'robert', 'bloch', 'author', 'psycho', \"n't\", 'paying', 'attention', 'scripted', 'tale', 'terror', 'absolutely', 'one', 'scariest', 'movies', 'i', 'ever', 'saw', 'kid', 'i', 'walk', 'miles', 'see', 'movie', 'usually', 'dark', 'i', 'emerged', 'theater', 'seeing', 'horror', 'movie', 'always', 'unnerving', 'particularly', 'one', 'when', 'i', 'opportunity', 'see', 'one', 'several', 'years', 'ago', 'videotape', 'always', 'last', 'resort', 'i', 'surprised', 'well', 'held', 'take', 'terror', 'test', 'watch', 'night', 'alone', 'then', 'tell', \"'s\", 'scary']]\n",
      "\n",
      "    Converted to indices:\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  26716 54787 11747 42597 56823 44504 56522 35443     6 38660     6 19649\n",
      "  13059 32448 16362 28658 22562 11267 31284 49647 42490 19584 52244 38050\n",
      "  54656 55214 39684 47539   525 35480 54643 36797 27371 56522 11184 54656\n",
      "  16984 58467 26716 47512   550 26274   551 35480]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0 54656 31252 23549 46268  6334  4006 43216 36933\n",
      "  40368  3846 48022 53845 54510   933 38777 47613 36463 26716 18755 47512\n",
      "  30145 26716 59147 35276 48241 36448 57978 13656 26716 17779 54660 48251\n",
      "  26180 36448  2214 57516 40104 38777 59772 26716 38904 48241 38777 48635\n",
      "  60913  1666 58591  2214 31237 45522 26716 53230 59630 25174 53815 54510\n",
      "  54543 59366 37704  2127 54690 54331     6 47627]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x = make_padded_inputs(train_words, word2ind)\n",
    "val_x = make_padded_inputs(val_words, word2ind)\n",
    "\n",
    "\n",
    "print(f\"\"\"Training example at indices 5 and 6:\\n{train_words[5:7]}\\n\n",
    "    Converted to indices:\\n{train_x[5:7, :]}\\n\"\"\")\n",
    "\n",
    "assert len(train_words[5]) == np.count_nonzero(train_x[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Load data into torch</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 d_feature, \n",
    "                 num_layers, \n",
    "                 hidden_size,\n",
    "                 n_outputs,\n",
    "                 bidirectional=False,\n",
    "                 dropout_rate=0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_feature)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.lstm = nn.LSTM(input_size=d_feature,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_outputs)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_data): \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input_data))\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        fc = self.fc(lstm_out[:,-1,:])\n",
    "        sigmoid = self.sigmoid(fc)\n",
    "   \n",
    "        return sigmoid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentClassifier(\n",
      "  (embedding): Embedding(61449, 128)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2ind)\n",
    "d_feature = 128\n",
    "hidden_size = 128\n",
    "n_outputs = 1\n",
    "num_layers = 1\n",
    "\n",
    "model = SentimentClassifier(\n",
    "                            vocab_size=vocab_size, \n",
    "                            d_feature=d_feature,  \n",
    "                            num_layers=num_layers, \n",
    "                            hidden_size=hidden_size, \n",
    "                            n_outputs=n_outputs).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Train model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader=train_loader,\n",
    "             val_loader=val_loader,\n",
    "             model=model,\n",
    "             optimizer=torch.optim.Adam(model.parameters(), lr=0.005),\n",
    "             criterion=nn.BCELoss(),\n",
    "             n_epochs=6):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:  \n",
    "            model.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "           \n",
    "        model.eval()\n",
    "            \n",
    "        val_losses = []\n",
    "        \n",
    "        \n",
    "        for val_inputs, val_labels in val_loader:\n",
    "\n",
    "            val_output = model(val_inputs)\n",
    "            val_loss = criterion(val_output.squeeze(), val_labels.float())\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}/{ n_epochs}\".format(),\n",
    "              f\"Time taken: {((time.time() - start_time) / 60):.2f} min\",\n",
    "              f\"Training Loss: {loss.item():.4f}\",\n",
    "              f\"Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "            \n",
    "    print(f\"Training completed in {(time.time() - start_time) / 60} min.\")\n",
    "    print(f\"Final loss: {loss}\\nValidation loss: {val_loss}\")\n",
    "    \n",
    "    return loss, val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "             n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
