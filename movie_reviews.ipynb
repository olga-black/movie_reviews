{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:turquoise\">Text classification with pytorch</span>\n",
    "\n",
    "\n",
    "An example of using natural language processing for sentiment analysis. <br> Building a binary classifier of movie reviews that will predict if a review is positive or negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Dataset:__ IMDB movie reviews from Kaggle<br>\n",
    "__Model:__ LSTM (?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:teal\">Todo:</span>\n",
    "\n",
    "- ~~Read dataset~~\n",
    "- ~~Preprocess text~~\n",
    "- ~~Split into train, validation, and test sets~~\n",
    "- ~~Convert text to indices and add paddings~~\n",
    "- ~~Make model~~\n",
    "- Make training function\n",
    "- Make evaluation function\n",
    "- Train\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Read the data and split it into training, cross-validation, and test sets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = {}\n",
    "        self.val = {}\n",
    "        self.test = {}\n",
    "        self.LABELS = {\"positive\":1, \"negative\": 0}\n",
    "        self.COUNT = {\"positive\": 0, \"negative\": 0}\n",
    "    \n",
    "    \n",
    "    def read_data(self):\n",
    "        \n",
    "        dataset = []\n",
    "        \n",
    "        with open (\"IMDB_Dataset.csv\", newline='') as f:\n",
    "            datareader = csv.reader(f, delimiter=',')\n",
    "            next(datareader, None)\n",
    "\n",
    "            for row in datareader:\n",
    "                dataset.append([row[0], self.LABELS[row[1]]])\n",
    "                self.COUNT[row[1]] += 1\n",
    "            \n",
    "            random.shuffle(dataset)\n",
    "                \n",
    "        return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(self,\n",
    "                      dataset,\n",
    "                      split=[int(50000*0.6), int(50000*0.2), int(50000*0.2)]):\n",
    "        \n",
    "        train, val, test = torch.utils.data.random_split(dataset,\n",
    "                                               split,\n",
    "                                               generator=torch.Generator().manual_seed(43))\n",
    "          \n",
    "            \n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = Reviews()\n",
    "data = rev.read_data()\n",
    "pos_count = rev.COUNT[\"positive\"]\n",
    "neg_count = rev.COUNT[\"negative\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I bought this movie hoping that it would be another great killer toy movie. I am a big fan of the Child\\'s Play series and was hoping to see the same here. Boy, was I wrong. Most of the movie was not the least bit scary, plus the only time we really see Pinocchio \"alive\" is the final few scenes of the film. The little girl in the film, her acting is so bad it\\'s almost laughable. Plus, the ending never showed what happened to the puppet or what made them put the little girl in a asylum or wherever she was at the end of the film. So, in my opinion this movie is the worst of the \"killer toy\" genre. If you want a good killer toy series, stick with the Child\\'s Play franchise. Pinocchio\\'s Revenge is a waste of money and time.', 0]\n"
     ]
    }
   ],
   "source": [
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"This movie came as a huge disappointment. The anime series ended with a relatively stupid plot twist and the rushed introduction of a pretty lame villain, but I expected Shamballa to tie up all the loose ends. Unfortunately, it didn't. It added more plot holes than it resolved, and confused more than it clarified. The animation and voice acting were great, but with an idiotic plot, dull setting (most of the movie doesn't even take place in dull WWII Earth rather than the Alchemy world), and disappointing ending (Ed is useless for the rest of his days in a world with no alchemy, and he ditches Winry?), it was altogether pretty lackluster. Do yourself a favor-- disregard the last half of the anime as well as this movie, and read the manga.\", 0]\n"
     ]
    }
   ],
   "source": [
    "train, val, test = rev.split_dataset(data)\n",
    "print(train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_and_y(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for review, label in data:\n",
    "        x.append(review)\n",
    "        y.append(label)\n",
    "    return x, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000\n",
      "I wish it were \"Last Dumb Thriller\". But thrillers are like that. They are like children: numerous, illogical, and often annoying. They want so desperately to be taken seriously but what is there to take seriously about a child's behaviour or a thriller's plot? Having seen this particular child - I mean... thriller - I understand why reviewers refer to it as \"a hitchcockian thriller\"; they might as well have called it \"idiotic\" for that's what \"hitchcockian\" means in the movie dictionary (look it up, if you don't believe me). Even the soundtrack is old-school Hollywood which is a mistake: it doesn't fit a late 70s film and makes it look phony. Besides, how dare they steal De Palma's idea of stealing from Hitchcock?! The story is absurd. Scheider's wife is killed, and her killers are never an issue. Instead, first his former employers follow him around, and later decide to kill him. Why do they decide to kill him? No explanation. Perhaps because the FBI is a dark, dark organization (\"X-Files\") which is very trigger-happy about knocking off its former employees for pension-funds reasons. Or perhaps because it's fashionable to want to kill Scheider in this movie; everyone seems to be after him. And while the poor unsuspecting viewer is trying to figure out the mystery by logically assuming that there is a major conspiracy, in reality the killer is... Janet Margolin! Yes, the woman occupying Scheider's living quarters; the one that briefly hinted she was \"depraved\". Why does she go after Scheider at precisely a time when his wife was murdered and he is feeling paranoid - and followed by his own ex-employers - and not a few years earlier or few years after the wife's murder? A pure hitchcockian (look it up again in the dictionary, in case you forgot what it means) coincidence. And how about that brilliant motive of hers...! Her grandmother was forced into prostitution when she was a fresh-off-the-boat 15 year-old virgin in NY, and then syphilisized by a bunch of horny Jewish men, one of whom - tah-dah! - is Scheider's grandfather. As a result, Margolin has been playing a hooker in her spare time (among other things) in order to kill off all the descendants of the men who so cruelly syphilisized her once-virginal grandmother. How hitchcockian (look it up) is that? The finale then shamelessly rips off the Mount Rushmore scene from \"North By Northwest\", except that the love-interest is a killer and she doesn't get saved. The movie also offers some dubious/off-kilter dialog and some not-so great acting. Check out the silly and obvious way in which Napier follows Scheider at the cemetery. Let's also not forget the moronic plot-device of Napier reaching for his jacket and holding his hand very suspiciously - but it wasn't a gun! How brilliant! Napier in the tower: now, there's another string of illogical behavioural patterns. J. Demme was, is, and always will be a director without style, without flair, and the man who directed \"Philadelphia\". Let's give him another Oscar! 0\n"
     ]
    }
   ],
   "source": [
    "train_x_raw, train_y = split_x_and_y(train)\n",
    "val_x_raw, val_y = split_x_and_y(val)\n",
    "test_x_raw, test_y = split_x_and_y(test)\n",
    "\n",
    "\n",
    "print(len(train_x_raw), len(train_y))\n",
    "print(train_x_raw[50], train_y[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Preprocess text</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review,\n",
    "               remove_stopwords=False, \n",
    "               remove_html=True, \n",
    "               remove_punct=False, \n",
    "               lowercase=False, \n",
    "               lemmatize=False,\n",
    "               maxlen=128):\n",
    "    \n",
    "    review = re.sub(r\"\\'\", \"'\", review)\n",
    "    review = re.sub(r\"\\x96\", \"-\", review)\n",
    "    \n",
    "    if remove_html:\n",
    "        review = re.sub(r'<.*>', ' ', review)\n",
    "    \n",
    "    review = word_tokenize(review)\n",
    "        \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        review = [w for w in review if w not in stop_words]\n",
    "        \n",
    "    if remove_punct:\n",
    "        contractions = [\"'ll\", \"'s\", \"n't\", \"'d\", \"'m\", \"'ve\", \"'re\"]\n",
    "        review = [w for w in review if w.isalnum() or w in contractions]\n",
    "    \n",
    "    if lowercase:\n",
    "        review = [w.lower() for w in review]\n",
    "        \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(w) for w in review]\n",
    "    \n",
    "    \n",
    "    return review[:maxlen]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [preprocess(review, \n",
    "                      lowercase=True, \n",
    "                      remove_punct=True,\n",
    "                      remove_stopwords=True\n",
    "                     ) \n",
    "           for review in train_x_raw]\n",
    "\n",
    "val_words = [preprocess(review, \n",
    "                    lowercase=True, \n",
    "                    remove_punct=True,\n",
    "                    remove_stopwords=True\n",
    "                   ) \n",
    "         for review in val_x_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'never', 'seen', 'original', 'death', 'wish', 'book', 'either', 'death', 'wish', 'i', 'film', 'however', 'death', 'wish', '3', 'interested', 'film', 'the', 'vigilante', 'paul', 'kersey', 'tried', 'visit', 'friend', 'charlie', 'visited', 'couple', 'minutes', 'died', 'charlie', \"n't\", 'pay', 'protection', 'huge', 'infamous', 'underground', 'gang', 'leaded', 'manny', 'franker', 'after', 'altercation', 'time', 'jail', 'kersey', 'learned', 'franker', 'fought', 'jail', 'agenda', 'make', 'new', 'york', 'city', 'hellfire', 'also', 'influence', 'sending', 'henchmen', 'set', 'crime', 'every', 'time', 'everywhere', 'everyday', 'due', 'chance', 'given', 'insp', 'richard', 'shriker', 'know', 'profile', 'well', 'like', 'much', 'kersey', 'decided', 'set', 'war', 'franker', 'gang', 'for', 'summary', 'i', 'ok', 'death', 'wish', '3', 'due', 'explanation', 'i', 'typed', 'first', 'the', 'cast', 'action', 'good', 'extreme', 'even', 'scene', 'i', \"n't\", 'like', 'straightedge', 'extreme', 'violence', 'could', 'best', 'short', 'description', 'movie'] \n",
      " ['the', 'first', 'time', 'i', 'saw', 'poster', 'i', 'stunned', 'tranquility', 'beauty', 'then', 'city', 'istanbul', 'haunting', 'mind', 'ever', 'since', 'we', 'deny', 'universal', 'problem', 'communication', 'loneliness', 'even', 'puts', 'us', 'far', 'towards', 'becomes', 'vicious', 'spiral', 'i', 'bet', 'mahmut', 'still', \"n't\", 'figure', 'way', 'living', 'end', 'that', \"'s\", 'stepped', 'room', 'try', 'find', 'answers', 'outer', 'world', 'coldness', 'landscape']\n",
      "113 \n",
      " 51\n"
     ]
    }
   ],
   "source": [
    "print(train_words[9592], '\\n', val_words[3029])\n",
    "print(len(train_words[9592]), '\\n', len(val_words[3029]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Convert text to indices and add paddings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary_dicts(preprocessed_data, pad_token='<PAD>', unk_token='<UNK>'):\n",
    "    vocab = set()\n",
    "    \n",
    "    for review in preprocessed_data:\n",
    "        for word in review:\n",
    "            vocab.add(word)\n",
    "    \n",
    "            \n",
    "    vocab_sorted = sorted(vocab)\n",
    "    word2ind = {word : i+2 for i, word in enumerate(vocab_sorted)}\n",
    "    ind2word = {i+2 : word for i, word in enumerate(vocab_sorted)}\n",
    "    \n",
    "    # Prepend the pad token\n",
    "    word2ind[pad_token] = 0\n",
    "    ind2word[0] = pad_token\n",
    "    \n",
    "    # Prepend the 'unknown' token\n",
    "    word2ind[unk_token] = 1\n",
    "    ind2word[1] = unk_token\n",
    "    \n",
    "    assert len(word2ind) == len(ind2word)\n",
    "\n",
    "    \n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_raw, val_x_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61697 61697\n",
      "37527 4150\n",
      "bonhomie closing\n"
     ]
    }
   ],
   "source": [
    "word2ind, ind2word = make_vocabulary_dicts(train_words)\n",
    "\n",
    "print(len(word2ind), len(ind2word))\n",
    "print(word2ind['never'], word2ind['awful'])\n",
    "print(ind2word[6700], ind2word[10582])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "71.15276666666666\n",
      "128\n",
      "71.3938\n"
     ]
    }
   ],
   "source": [
    "print(np.max([len(x) for x in train_words]))\n",
    "print(np.mean([len(x) for x in train_words]))\n",
    "\n",
    "print(np.max([len(x) for x in val_words]))\n",
    "print(np.mean([len(x) for x in val_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_inputs(preprocessed_data, \n",
    "                       vocab, \n",
    "                       padded_length=128,\n",
    "                       pad_token='<PAD>',\n",
    "                       unk_token='<UNK>'\n",
    "                      ):\n",
    "    \n",
    "    num_lines = len(preprocessed_data)\n",
    "    pad = vocab[pad_token]\n",
    "    \n",
    "    unpadded_lengths = np.zeros(num_lines, dtype='int64')\n",
    "    \n",
    "    inputs = np.full((num_lines, padded_length), pad)\n",
    "    \n",
    "    for i, review in enumerate(preprocessed_data):\n",
    "        for j, word in enumerate(review):    \n",
    "            inputs[i, j] = vocab.get(word, vocab[unk_token])\n",
    "        unpadded_lengths[i] = j+1\n",
    "            \n",
    "    return inputs, unpadded_lengths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example at index 10:\n",
      "['this', 'movie', 'came', 'huge', 'disappointment', 'the', 'anime', 'series', 'ended', 'relatively', 'stupid', 'plot', 'twist', 'rushed', 'introduction', 'pretty', 'lame', 'villain', 'i', 'expected', 'shamballa', 'tie', 'loose', 'ends', 'unfortunately', \"n't\", 'it', 'added', 'plot', 'holes', 'resolved', 'confused', 'clarified', 'the', 'animation', 'voice', 'acting', 'great', 'idiotic', 'plot', 'dull', 'setting', 'movie', \"n't\", 'even', 'take', 'place', 'dull', 'wwii', 'earth', 'rather', 'alchemy', 'world', 'disappointing', 'ending', 'ed', 'useless', 'rest', 'days', 'world', 'alchemy', 'ditches', 'winry', 'altogether', 'pretty', 'lackluster', 'do', 'favor', 'disregard', 'last', 'half', 'anime', 'well', 'movie', 'read', 'manga']\n",
      "\n",
      "    Converted to indices:\n",
      "[55008 36469  8312 26453 15389 54830  2659 48705 18004 45114 52767 41651\n",
      " 56803 46979 28373 42635 31043 58915 26766 19107 48952 55243 32560 18028\n",
      " 57518 36957 28693  1251 41651 25904 45601 11588 10331 54830  2651 59151\n",
      "  1172 23618 26875 41651 16830 48774 36469 36957 18752 53985 41457 16830\n",
      " 60964 17146 44281  1897 60759 15387 18012 17276 58178 45653 13749 60759\n",
      "  1897 15852 60433  2183 42635 30933 15951 19845 15708 31267 24393  2659\n",
      " 59893 36469 44418 33607     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      " \n",
      "    Unpadded length of the example:\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "train_x, train_lengths = make_padded_inputs(train_words, word2ind)\n",
    "val_x, val_lengths = make_padded_inputs(val_words, word2ind)\n",
    "\n",
    "\n",
    "print(f\"\"\"Training example at index 10:\\n{train_words[10]}\\n\n",
    "    Converted to indices:\\n{train_x[10, :]}\\n \n",
    "    Unpadded length of the example:\\n{train_lengths[10]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Load data into torch</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Create model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 d_feature, \n",
    "                 num_layers, \n",
    "                 hidden_size,\n",
    "                 n_outputs,\n",
    "                 bidirectional=False,\n",
    "                 dropout_rate=0.9):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_feature)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.lstm = nn.LSTM(input_size=d_feature,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, n_outputs)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, \n",
    "                input_data, \n",
    "                unpadded_lengths, \n",
    "                padding_value=0):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input_data))\n",
    "        packed_embedded = pack_padded_sequence(embedded,\n",
    "                                               lengths=torch.from_numpy(unpadded_lengths),\n",
    "                                               enforce_sorted=False)\n",
    "        lstm_out, _ = self.lstm(packed_embedded)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out,\n",
    "                                          padding_value=padding_value,\n",
    "                                          total_length=128)\n",
    "        fc = self.fc(lstm_out.view(len(input_data), -1))\n",
    "        logsoftmax = self.logsoftmax(fc).view(batch_size, -1)\n",
    "        \n",
    "        \n",
    "        return logsoftmax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentClassifier(\n",
      "  (embedding): Embedding(61697, 128)\n",
      "  (dropout): Dropout(p=0.9, inplace=False)\n",
      "  (lstm): LSTM(128, 128, num_layers=2)\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (logsoftmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2ind)\n",
    "d_feature = 128\n",
    "hidden_size = 128\n",
    "n_outputs = 2\n",
    "num_layers = 2\n",
    "\n",
    "model = SentimentClassifier(\n",
    "                            vocab_size=vocab_size, \n",
    "                            d_feature=d_feature,  \n",
    "                            num_layers=num_layers, \n",
    "                            hidden_size=hidden_size, \n",
    "                            n_outputs=n_outputs)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
