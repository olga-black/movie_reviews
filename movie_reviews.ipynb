{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:turquoise\">Text classification with pytorch</span>\n",
    "\n",
    "\n",
    "An example of using natural language processing for sentiment analysis. <br> Building a binary classifier of movie reviews that will predict if a review is positive or negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Dataset:__ IMDB movie reviews from Kaggle<br>\n",
    "__Model:__ bag-of-words + RNN(?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:teal\">Todo:</span>\n",
    "\n",
    "- ~~Read dataset~~\n",
    "- ~~Preprocess text~~\n",
    "- ~~Split into train, validation, and test sets~~\n",
    "- Convert text to indices and add paddings\n",
    "- Make model\n",
    "- Make training function\n",
    "- Make evaluation function\n",
    "- Train\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Read the data and split it into training, cross-validation, and test sets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = {}\n",
    "        self.val = {}\n",
    "        self.test = {}\n",
    "        self.LABELS = {\"positive\":1, \"negative\": 0}\n",
    "        self.COUNT = {\"positive\": 0, \"negative\": 0}\n",
    "    \n",
    "    \n",
    "    def read_data(self):\n",
    "        \n",
    "        dataset = []\n",
    "        \n",
    "        with open (\"IMDB_Dataset.csv\", newline='') as f:\n",
    "            datareader = csv.reader(f, delimiter=',')\n",
    "            next(datareader, None)\n",
    "\n",
    "            for row in datareader:\n",
    "                dataset.append([row[0], self.LABELS[row[1]]])\n",
    "                self.COUNT[row[1]] += 1\n",
    "            \n",
    "            random.shuffle(dataset)\n",
    "                \n",
    "        return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_dataset(self,\n",
    "                      dataset,\n",
    "                      split=[int(50000*0.6), int(50000*0.2), int(50000*0.2)]):\n",
    "        \n",
    "        train, val, test = torch.utils.data.random_split(dataset,\n",
    "                                               split,\n",
    "                                               generator=torch.Generator().manual_seed(43))\n",
    "          \n",
    "            \n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = Reviews()\n",
    "data = rev.read_data()\n",
    "pos_count = rev.COUNT[\"positive\"]\n",
    "neg_count = rev.COUNT[\"negative\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This version of \"The Magic Flute\" is not only the worst production of Mozart\\'s great opera that I have ever seen, it is also the worst video production I have seen of any opera. This is not a movie version of \"The Magic Flute.\" It is a filmed performance and it is not a good performance and it was not filmed very well. You can pick any other available DVD of this opera and I guarantee it will be better than this one. My preference is for the version conducted by James Levine with sets by David Hockney.', 0]\n"
     ]
    }
   ],
   "source": [
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Over the years I've seen a bunch of these straight to video Segal movies, and every one holds the same amount of entertainment; unfortanetley, the entertainment level is at a low. Sure, the action sequences were amusing, but that was pretty much it. Seagal was really in his prime when he did movies like; Under Siege, Under Siege 2, and Executive Decision(at least on the action standpoint), but during the past ten years, these types of movies that star Segal really do not meet his past qualifications. On the more positive side, the movie did make good use of time, like some of the action sequences and use of wit. Just when the movie seemed to just drag on, a pretty cool action scene brought it up out of the gutter. I honestly believe that more of Segal's movies would do better if he wasn't the only one that fans recognize in the movie. Supporting actors and actresses are a very important thing, and if his current movies had this known supporting actors and actresses, maybe the movie will get more popular results.\", 0]\n"
     ]
    }
   ],
   "source": [
    "train, val, test = rev.split_dataset(data)\n",
    "print(train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_and_y(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for review, label in data:\n",
    "        x.append(review)\n",
    "        y.append(label)\n",
    "    return x, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000\n",
      "Peter O'Toole is a treat to watch in roles where the lines he speaks are good and offer a chance for him to swagger in drunken stupor. The lovely Susannah York provides a good foil for O'Toole's dramatic presence. I saw the film twice over a period of 20 years--on both occasions with the name \"Brotherly love\". \"Country dance\" is a rather farcical and inappropriate title for this movie, wherever it was released as such. 1\n"
     ]
    }
   ],
   "source": [
    "train_x_raw, train_y = split_x_and_y(train)\n",
    "val_x_raw, val_y = split_x_and_y(val)\n",
    "test_x_raw, test_y = split_x_and_y(test)\n",
    "\n",
    "\n",
    "print(len(train_x_raw), len(train_y))\n",
    "print(train_x_raw[50], train_y[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Preprocess text</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review,\n",
    "               remove_stopwords=False, \n",
    "               remove_html=True, \n",
    "               remove_punct=False, \n",
    "               lowercase=False, \n",
    "               lemmatize=False,\n",
    "               maxlen=128):\n",
    "    \n",
    "    review = re.sub(r\"\\'\", \"'\", review)\n",
    "    review = re.sub(r\"\\x96\", \"-\", review)\n",
    "    \n",
    "    if remove_html:\n",
    "        review = re.sub(r'<.*>', ' ', review)\n",
    "    \n",
    "    review = word_tokenize(review)\n",
    "        \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        review = [w for w in review if w not in stop_words]\n",
    "        \n",
    "    if remove_punct:\n",
    "        contractions = [\"'ll\", \"'s\", \"n't\", \"'d\", \"'m\", \"'ve\", \"'re\"]\n",
    "        review = [w for w in review if w.isalnum() or w in contractions]\n",
    "    \n",
    "    if lowercase:\n",
    "        review = [w.lower() for w in review]\n",
    "        \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        review = [lemmatizer.lemmatize(w) for w in review]\n",
    "    \n",
    "    \n",
    "    return review[:maxlen]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [preprocess(review, \n",
    "                      lowercase=True, \n",
    "                      remove_punct=True,\n",
    "                      remove_stopwords=True\n",
    "                     ) \n",
    "           for review in train_x_raw]\n",
    "\n",
    "val_x = [preprocess(review, \n",
    "                    lowercase=True, \n",
    "                    remove_punct=True,\n",
    "                    remove_stopwords=True\n",
    "                   ) \n",
    "         for review in val_x_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['probably', 'worst', 'movie', 'i', 'ever', 'seen', 'it', 'cheesily', 'filmed', 'focus', 'even', 'supposed', 'real', 'crew', 'coming', 'hollywood', 'make', 'movie', 'no', 'cinematic', 'significance', 'whatsoever', 'i', 'could', 'take', 'back', 'almost', '1', 'hours', 'i', 'spent', 'watching', 'film', 'i', 'would', 'feel', 'much', 'better', 'worst', 'movie', 'i', 'ever', 'seen', 'positive', 'side', 'like', 'one', 'scene', 'visuals', \"n't\", 'bad', 'looking', 'do', 'rent'] \n",
      " ['emma', 'favourite', 'jane', 'austen', 'novel', 'emma', 'despite', 'flaws', 'readers', 'forgive', 'love', 'relationship', 'mr', 'knightley', 'warm', 'familiar', 'respectful', 'playful', 'generating', 'warm', 'fuzzy', 'romantic', 'excitement', 'mr', 'knightley', 'perfect', 'man', 'emma', 'close', 'could', 'get', 'times', 'independent', 'clever', 'confident', 'woman', 'remember', '21', 'sure', 'matured', 'grown', 'flaws', 'who', \"n't\", 'want', 'emma', 'who', \"n't\", 'want', 'told', 'mr', 'knightley', 'this', 'version', 'emma', 'gives', 'sense', 'things', 'i', 'love', 'emma', 'i', 'could', \"n't\", 'even', 'finish', 'watching', 'i', 'found', 'awful', 'i', 'could', \"n't\", 'see', 'warm', 'generous', 'side', 'emma', 'drives', 'reader', 'love', 'the', 'patience', 'warmth', 'shows', 'father', 'closeness', 'mrs', 'weston', 'demonstrates', 'willingness', 'put', 'friend', \"'s\", 'happiness', 'sacrifices', 'equal', 'companion', 'household', 'forwarding', 'miss', 'taylors', 'marriage', 'mr', 'woodhouse', \"'s\", 'character', 'adaptation', 'appears', 'bizarre', 'rather', 'quaint', 'elderly', 'bit', 'trying', 'really', 'horrible', 'i', 'ca', \"n't\", 'understand', 'anyone', 'truly', 'like', 'novel', 'emma', 'could', 'like']\n",
      "54 \n",
      " 128\n"
     ]
    }
   ],
   "source": [
    "print(train_x[9592], '\\n', val_x[3029])\n",
    "print(len(train_x[9592]), '\\n', len(val_x[3029]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:teal\">Convert text to indices and add paddings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary_dicts(preprocessed_data, pad_token='<PAD>', unk_token='<UNK>'):\n",
    "    vocab = set()\n",
    "    \n",
    "    for review in preprocessed_data:\n",
    "        for word in review:\n",
    "            vocab.add(word)\n",
    "    \n",
    "            \n",
    "    vocab_sorted = sorted(vocab)\n",
    "    word2ind = {word : i+2 for i, word in enumerate(vocab_sorted)}\n",
    "    ind2word = {i+2 : word for i, word in enumerate(vocab_sorted)}\n",
    "    \n",
    "    # Prepend the pad token\n",
    "    word2ind[pad_token] = 0\n",
    "    ind2word[0] = pad_token\n",
    "    \n",
    "    # Prepend the 'unknown' token\n",
    "    word2ind[unk_token] = 1\n",
    "    ind2word[1] = unk_token\n",
    "    \n",
    "    assert len(word2ind) == len(ind2word)\n",
    "\n",
    "    \n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del train_x_raw, val_x_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61724 61724\n",
      "37585 4171\n",
      "bonbons clin\n"
     ]
    }
   ],
   "source": [
    "word2ind, ind2word = make_vocabulary_dicts(train_x)\n",
    "print(len(word2ind), len(ind2word))\n",
    "print(word2ind['never'], word2ind['awful'])\n",
    "print(ind2word[6700], ind2word[10582])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "71.39476666666667\n",
      "128\n",
      "70.9034\n"
     ]
    }
   ],
   "source": [
    "print(np.max([len(x) for x in train_x]))\n",
    "print(np.mean([len(x) for x in train_x]))\n",
    "\n",
    "print(np.max([len(x) for x in val_x]))\n",
    "print(np.mean([len(x) for x in val_x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_inputs(preprocessed_data, \n",
    "                       vocab, \n",
    "                       padded_length=128,\n",
    "                       pad_token='<PAD>',\n",
    "                       unk_token='<UNK>'\n",
    "                      ):\n",
    "    \n",
    "    num_lines = len(preprocessed_data)\n",
    "    pad = vocab[pad_token]\n",
    "    \n",
    "    inputs = np.full((num_lines, padded_length), pad)\n",
    "    \n",
    "    for i, review in enumerate(preprocessed_data):\n",
    "        for j, word in enumerate(review):    \n",
    "            inputs[i, j] = vocab.get(word, vocab[unk_token])\n",
    "            \n",
    "    mask = np.where(inputs==0, 0, 1)\n",
    "            \n",
    "    return inputs, mask\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example at index 10:\n",
      "['over', 'years', 'i', \"'ve\", 'seen', 'bunch', 'straight', 'video', 'segal', 'movies', 'every', 'one', 'holds', 'amount', 'entertainment', 'unfortanetley', 'entertainment', 'level', 'low', 'sure', 'action', 'sequences', 'amusing', 'pretty', 'much', 'seagal', 'really', 'prime', 'movies', 'like', 'under', 'siege', 'under', 'siege', '2', 'executive', 'decision', 'least', 'action', 'standpoint', 'past', 'ten', 'years', 'types', 'movies', 'star', 'segal', 'really', 'meet', 'past', 'qualifications', 'on', 'positive', 'side', 'movie', 'make', 'good', 'use', 'time', 'like', 'action', 'sequences', 'use', 'wit', 'just', 'movie', 'seemed', 'drag', 'pretty', 'cool', 'action', 'scene', 'brought', 'gutter', 'i', 'honestly', 'believe', 'segal', \"'s\", 'movies', 'would', 'better', \"n't\", 'one', 'fans', 'recognize', 'movie', 'supporting', 'actors', 'actresses', 'important', 'thing', 'current', 'movies', 'known', 'supporting', 'actors', 'actresses', 'maybe', 'movie', 'get', 'popular', 'results']\n",
      "\n",
      "    Converted to indices:\n",
      "[39424 61189 26788     7 48408  7884 52504 58818 48419 36523 18845 38850\n",
      " 25921  2427 18273 57530 18273 31836 32720 53391  1206 48646  2465 42691\n",
      " 36585 48242 44485 42756 36523 32030 57248 49666 57248 49666   374 19059\n",
      " 14016 31515  1206 51952 40235 54596 61189 56878 36523 51980 48419 44485\n",
      " 34729 40235 43703 38835 42192 49632 36506 33398 23275 58194 55304 32030\n",
      "  1206 48646 58194 60514 29638 36506 48403 16509 42691 12118  1206 47790\n",
      "  7590 24211 26788 26087  5416 48419     6 36523 60832  5740 37002 38850\n",
      " 19681 44669 36506 53347  1231  1233 27230 54983 13272 36523 30542 53347\n",
      "  1231  1233 34320 36506 22666 42080 45723     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      " \n",
      "    Mask of the example:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_mask = make_padded_inputs(train_x, word2ind)\n",
    "val_inputs, val_mask = make_padded_inputs(val_x, word2ind)\n",
    "\n",
    "\n",
    "print(f\"\"\"Training example at index 10:\\n{train_x[10]}\\n\n",
    "    Converted to indices:\\n{train_inputs[10, :]}\\n \n",
    "    Mask of the example:\\n{train_mask[10, :]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
